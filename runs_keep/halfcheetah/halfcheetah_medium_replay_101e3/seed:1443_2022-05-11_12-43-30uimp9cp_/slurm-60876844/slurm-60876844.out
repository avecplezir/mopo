Can't load '/usr/local/software/slurm/slurm-20.11.9-rhel8/lib64/perl5/auto/Slurm/Slurm.so' for module Slurm: libperl.so.5.26: cannot open shared object file: No such file or directory at /usr/lib64/perl5/DynaLoader.pm line 190, <DATA> line 602.
 at /usr/local/software/slurm/current-rhel8/bin/generate_pbs_nodefile line 18.
Compilation failed in require at /usr/local/software/slurm/current-rhel8/bin/generate_pbs_nodefile line 18, <DATA> line 602.
BEGIN failed--compilation aborted at /usr/local/software/slurm/current-rhel8/bin/generate_pbs_nodefile line 18, <DATA> line 602.
WARNING: Logging before flag parsing goes to stderr.
W0511 12:43:22.381970 47609441502080 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.
No module named 'flow'
Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.
No module named 'carla'
2022-05-11 12:43:28,199	INFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2022-05-11_12-43-28_198054_7816/logs.
2022-05-11 12:43:28,351	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:26891 to respond...
2022-05-11 12:43:29,352	INFO services.py:414 -- Failed to connect to the redis server, retrying.
2022-05-11 12:43:29,353	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:26891 to respond...
2022-05-11 12:43:29,476	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:49835 to respond...
2022-05-11 12:43:29,477	INFO services.py:806 -- Starting Redis shard with 10.0 GB max memory.
2022-05-11 12:43:29,528	INFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2022-05-11_12-43-28_198054_7816/logs.
2022-05-11 12:43:29,529	WARNING services.py:1294 -- Warning: Capping object memory store to 20.0GB. To increase this further, specify `object_store_memory` when calling ray.init() or ray start.
2022-05-11 12:43:29,529	INFO services.py:1442 -- Starting the Plasma object store with 20.0 GB memory using /dev/shm.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0511 12:43:29.607725  7816 io.cc:168] Connection to IPC socket failed for pathname /tmp/ray/session_2022-05-11_12-43-28_198054_7816/sockets/plasma_store, retrying 300 more times
2022-05-11 12:43:30,123	INFO tune.py:61 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run()
2022-05-11 12:43:30,123	INFO tune.py:232 -- Starting a new experiment.
W0511 12:43:30.157842 47609441502080 deprecation_wrapper.py:119] From /rds/user/ajc348/hpc-work/mopo/.env/lib/python3.6/site-packages/ray/tune/logger.py:131: The name tf.VERSION is deprecated. Please use tf.version.VERSION instead.

W0511 12:43:30.158184 47609441502080 deprecation_wrapper.py:119] From /rds/user/ajc348/hpc-work/mopo/.env/lib/python3.6/site-packages/ray/tune/logger.py:136: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

W0511 12:48:58.498700 47609441502080 deprecation_wrapper.py:119] From /rds/user/ajc348/hpc-work/mopo/.env/lib/python3.6/site-packages/ray/tune/logger.py:114: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.

2022-05-11 12:55:41,304	WARNING util.py:64 -- The `save_to_disk` operation took 0.8288676738739014 seconds to complete, which may be a performance bottleneck.
2022-05-11 12:55:41,307	WARNING util.py:64 -- The `process_trial` operation took 0.841594934463501 seconds to complete, which may be a performance bottleneck.
2022-05-11 12:57:59,062	WARNING util.py:64 -- The `experiment_checkpoint` operation took 0.10211491584777832 seconds to complete, which may be a performance bottleneck.
2022-05-11 13:03:14,935	WARNING util.py:64 -- The `save_to_disk` operation took 0.4288301467895508 seconds to complete, which may be a performance bottleneck.
2022-05-11 13:03:14,938	WARNING util.py:64 -- The `process_trial` operation took 0.43964099884033203 seconds to complete, which may be a performance bottleneck.
2022-05-11 13:10:44,837	WARNING util.py:64 -- The `save_to_disk` operation took 0.4219245910644531 seconds to complete, which may be a performance bottleneck.
2022-05-11 13:10:44,839	WARNING util.py:64 -- The `process_trial` operation took 0.43308520317077637 seconds to complete, which may be a performance bottleneck.
2022-05-11 13:18:14,147	WARNING util.py:64 -- The `save_to_disk` operation took 0.5263919830322266 seconds to complete, which may be a performance bottleneck.
2022-05-11 13:18:14,149	WARNING util.py:64 -- The `process_trial` operation took 0.5382828712463379 seconds to complete, which may be a performance bottleneck.
2022-05-11 13:25:40,442	WARNING util.py:64 -- The `save_to_disk` operation took 0.40952467918395996 seconds to complete, which may be a performance bottleneck.
2022-05-11 13:25:40,446	WARNING util.py:64 -- The `process_trial` operation took 0.42156100273132324 seconds to complete, which may be a performance bottleneck.
2022-05-11 13:33:04,678	WARNING util.py:64 -- The `save_to_disk` operation took 0.4411776065826416 seconds to complete, which may be a performance bottleneck.
2022-05-11 13:33:04,680	WARNING util.py:64 -- The `process_trial` operation took 0.45180702209472656 seconds to complete, which may be a performance bottleneck.
2022-05-11 13:38:15,611	WARNING util.py:64 -- The `experiment_checkpoint` operation took 0.1043858528137207 seconds to complete, which may be a performance bottleneck.
2022-05-11 13:40:30,105	WARNING util.py:64 -- The `save_to_disk` operation took 0.41598057746887207 seconds to complete, which may be a performance bottleneck.
2022-05-11 13:40:30,107	WARNING util.py:64 -- The `process_trial` operation took 0.4265592098236084 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 60876844 ON gpu-e-8 CANCELLED AT 2022-05-11T13:42:28 DUE TO TIME LIMIT ***
